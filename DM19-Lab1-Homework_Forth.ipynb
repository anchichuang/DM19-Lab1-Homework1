{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information_Forth part\n",
    "\n",
    "Name: 莊安琦\n",
    "\n",
    "Student ID: 108064532\n",
    "\n",
    "GitHub ID: anchichuang"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Reduce the dimension:\n",
    "    I think we can remove meaningless terms, because we count all the terms, while some of them are meaningless. So we can remove some of them from the document term matrix.\n",
    "\n",
    "2. Do stemming\n",
    "    I think in data classification part, we can do stemming, and it will make the word vector more meaningful.\n",
    "\n",
    "\n",
    "3. Heatmap\n",
    "    We plot all the term-document matrix in Heatmap, but I think it wan't useful. Because tha data was too sparse. Maybe we can use the sentiment or categories in y-axis and the meaningful word in x-axis.\n",
    "    \n",
    "   \n",
    "\n",
    "For PCA part, I think PCA should do at the last step of feature engineering. Because it is hard to interpret the result from PCA. Better idea is to do RandomForest first for example, and it is straightforward to explain the relationship among features and labels. Once you walk to the end and have no idea to deal with these remaining features, then you do PCA finally.\n",
    "\n",
    "And the last part calculate the cosine similarity based on the CountVect transform. I think it is not suitable because some punctuations might affect the result. We had better do some lowercasing, stemming and removing punctuations.\n",
    "\n",
    "\n",
    "In the data preprocessing part, the WORD VECTOR generated in this way cannot preserve the relationship between each data, and it will also makes the dimension too large, and then getting the worse performance.\n",
    "I think in the text classification task, we can do stemming, remove stop word, and then use the neuron network to generate more meaniful vector such like document emedding for the sentence, the document emedding can preserve the relationship between each sentence,  that will be a nice Data preprocessing.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
